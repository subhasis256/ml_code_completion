\section{Setup and Model}
\label{sec:model}

\noindent
One of our objectives of learning based code prediction is to do away with the
tedious process of building grammar based rules for different languages.
We treat codes in the training set in a language agonostic way. The first step
is to build a dictionary of tokens or words that can occur in the code, that we
can take as input to predict the next token. We build this dictionary by
reading all code, and treating each consecutive set of alphanumeric characters
and (\_), or each conseuctive set of special characters --- characters other
than alphanumeric and (\_), separated by new line or space, as one token.
Thus, for example, the code {\tt int my\_var = 0x10;} will be tokenized into five
tokens: {\tt int}, {\tt my\_var}, {\tt =}, {\tt 0x10} and {\tt ;}. Given a sequence of
tokens, we then want to predict the next token.

The dictionary of tokens constructed as above is not complete, since new code
may contain new tokens that are not present in the training data. Moreover,
many of the tokens in the training data may be specific to a few files and may
never occur again. To address these issues, we divide the tokens into two
categories:

a) {\it Key tokens}: A subset of $K$ most frequent tokens are categorized as
{\it key tokens}. Not surprisingly, many of
these frequent tokens are keywords for the language the project is written in,
or words that tend to occur often in that
particular project.
For example, some of the in linux kernel are:
\texttt{struct}, \texttt{;}, and \texttt{dev}, out of which the first and second
are keywords in C, and the third is a token frequently used to denote device
objects in Linux. Note that while several language keywords do end up being part
of the set of key tokens, we {\it do not} manually curate the list of key tokens
to ensure that they contain only language specific keywords.

b) {\it Positional Tokens}: While key tokens constitute a major fraction of all
tokens ($\approx$ 60\% with 2000 key tokens), the remainder of tokens are rarely
seen (such as names of variables, macros etc.). Moreover, we will also like our
learning algorithm to be able to autocomplete completely new variable names. For
example, given many examples of the form {\tt for (int TOKEN = 0; TOKEN < n;
TOKEN++)} (all with different values of {\tt TOKEN}), our algorithm should be
able to autocomplete {\tt for (int myIterName = 0;} with {\tt myIterName}. This
capability can not be achieved by merely trying to autocomplete between a set of
pre-defined key tokens. Thus, we also define {\it positional tokens} as follows.

Given a sequence of tokens --- an incomplete piece of code, we replace each
token which is not a key token by a string of the form {\tt
POS\_TOK\_ii}, where {\tt ii} is the {\it position} of that token
within that sequence. A non-keyword token which is repeated multiple times in a
sequence is assigned the index corresponding to its first appearance. For
example, given the sequence of tokens {\tt[for, int, myVarName, =, 0, ;,
myVarName, <, n, ;, myVarName, ++]}, where {\tt myVarName} is not a key token,
we construct the new sequence {\tt[for, int, POS\_TOK\_2, =, 0, ;, POS\_TOK\_2,
<, n, ;, POS\_TOK\_2, ++]}. In case the token to be predicted is not a key token
but has appeared in the window, it is also replaced by the corresponding
positional token string. If the target has not appeared before in the window, it
is assumed to be a special token {\tt UNKNOWN}. However, as we describe in
Section~\ref{sec:memoryless}, we actually ignore such windows since in many such
cases the token is actually a hereto unseen token.

Such an encoding is advantageous since now the set of prediction targets to
choose from is simply the union of the key tokens and the positional token
strings (of the form {\tt POS\_TOKEN\_ii}). In case of a fixed window size of
$W$ and a fixed number of key tokens $K$, it can be seen that the total number
of prediction targets is $K+W+1$ ($K$ key tokens, $W$ positional tokens, and 1
unknown token), i.e., a {\it constant}. This immediately opens up the
possibility of applying simple models such as softmax based linear/non-linear
classifiers on this data. In Section~\ref{sec:memoryless} we describe some such
models.

%Given a sequence of tokens --- an incomplete piece of code, we then construct a
%sequence of identifiers by replacing
%key tokens by a unique key token identifier between $1$ and $K$ that identifies
%the key token.
%We replace the remaining tokens by positional identifiers --- if the $i^{th}$
%token in the sequence is not a key token, we replace it by identifier $K+i$. Our
%prediction would then be either a key token identifier or a positional token
%identifier.
%
%Using a finite dictionary and positional tokens as described above gives rise
%to cases where the next token after a sequence of tokens is neither a key token
%nor a positional token. For now, we ignore these cases, since many of these
%cases are actually new tokens that we would not have been able to predict even
%with a more complete dictionary.

%\begin{itemize}
%  \item word to vector
%\end{itemize}
